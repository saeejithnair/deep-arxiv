import type { Paper, Category } from './types';

// Enhanced paper data with categories and summaries
export const papers: Paper[] = [
  {
    id: "attention-is-all-you-need",
    arxivId: "1706.03762",
    title: "Attention Is All You Need",
    authors: ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin"],
    abstract: "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.",
    views: "2.8M",
    citations: "85.3k",
    category: "Machine Learning",
    field: "Computer Science",
    publishedDate: "12 Jun 2017",
    summary: "This paper introduces the Transformer architecture, which revolutionized natural language processing by relying entirely on attention mechanisms instead of recurrent or convolutional layers.",
    methodology: "The paper proposes a novel neural network architecture called the Transformer that uses multi-head self-attention mechanisms to process sequential data."
  },
  {
    id: "bert-pre-training",
    arxivId: "1810.04805",
    title: "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    authors: ["Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova"],
    abstract: "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.",
    views: "1.9M",
    citations: "72.1k",
    category: "Machine Learning",
    field: "Computer Science",
    publishedDate: "11 Oct 2018",
    summary: "BERT introduces bidirectional pre-training for language models, achieving state-of-the-art results on eleven natural language processing tasks.",
    methodology: "Uses masked language modeling and next sentence prediction for pre-training bidirectional transformers on large text corpora."
  },
  {
    id: "gpt-improving-understanding",
    arxivId: "1706.01427",
    title: "Improving Language Understanding by Generative Pre-Training",
    authors: ["Alec Radford", "Karthik Narasimhan", "Tim Salimans", "Ilya Sutskever"],
    abstract: "Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, and semantic similarity assessment. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately.",
    views: "1.5M",
    citations: "28.4k",
    category: "Machine Learning",
    field: "Computer Science",
    publishedDate: "11 Jun 2018",
    summary: "The original GPT paper demonstrating how generative pre-training on unlabeled text can significantly improve performance on downstream NLP tasks.",
    methodology: "Combines unsupervised pre-training on a language modeling objective with supervised fine-tuning on specific tasks."
  },
  {
    id: "resnet-deep-residual",
    arxivId: "1512.03385",
    title: "Deep Residual Learning for Image Recognition",
    authors: ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"],
    abstract: "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions.",
    views: "2.1M",
    citations: "156.8k",
    category: "Computer Vision",
    field: "Computer Science",
    publishedDate: "10 Dec 2015",
    summary: "ResNet introduces skip connections to enable training of very deep neural networks, winning ImageNet 2015 and revolutionizing computer vision.",
    methodology: "Introduces residual blocks with skip connections that allow gradients to flow directly through the network, enabling training of networks with hundreds of layers."
  },
  {
    id: "gan-generative-adversarial",
    arxivId: "1406.2661",
    title: "Generative Adversarial Networks",
    authors: ["Ian J. Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"],
    abstract: "We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G.",
    views: "1.8M",
    citations: "45.2k",
    category: "Machine Learning",
    field: "Computer Science",
    publishedDate: "10 Jun 2014",
    summary: "GANs introduce a revolutionary approach to generative modeling through adversarial training between generator and discriminator networks.",
    methodology: "Two neural networks compete in a game-theoretic framework where the generator learns to create realistic data while the discriminator learns to distinguish real from fake data."
  },
  {
    id: "vit-image-worth",
    arxivId: "2010.11929",
    title: "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
    authors: ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"],
    abstract: "While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place.",
    views: "892k",
    citations: "18.7k",
    category: "Computer Vision",
    field: "Computer Science",
    publishedDate: "22 Oct 2020",
    summary: "Vision Transformer (ViT) demonstrates that pure transformers can achieve excellent performance on image classification without convolutions.",
    methodology: "Splits images into patches, linearly embeds them, and processes the sequence with a standard Transformer encoder."
  },
  {
    id: "adam-optimizer",
    arxivId: "1412.6980",
    title: "Adam: A Method for Stochastic Optimization",
    authors: ["Diederik P. Kingma", "Jimmy Ba"],
    abstract: "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters.",
    views: "1.2M",
    citations: "89.7k",
    category: "Machine Learning",
    field: "Computer Science",
    publishedDate: "22 Dec 2014",
    summary: "Adam optimizer combines the advantages of AdaGrad and RMSprop, becoming one of the most widely used optimization algorithms in deep learning.",
    methodology: "Computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients."
  },
  {
    id: "dropout-preventing",
    arxivId: "1207.0580",
    title: "Dropout: A Simple Way to Prevent Neural Networks from Overfitting",
    authors: ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"],
    abstract: "Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time.",
    views: "756k",
    citations: "67.3k",
    category: "Machine Learning",
    field: "Computer Science",
    publishedDate: "15 Jul 2012",
    summary: "Dropout introduces a simple yet effective regularization technique that randomly sets neurons to zero during training to prevent overfitting.",
    methodology: "Randomly sets a fraction of input units to 0 at each update during training time, which helps prevent overfitting."
  },
  {
    id: "batch-normalization",
    arxivId: "1502.03167",
    title: "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
    authors: ["Sergey Ioffe", "Christian Szegedy"],
    abstract: "Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities.",
    views: "923k",
    citations: "54.8k",
    category: "Machine Learning",
    field: "Computer Science",
    publishedDate: "11 Feb 2015",
    summary: "Batch normalization normalizes layer inputs to accelerate training and improve the stability of deep neural networks.",
    methodology: "Normalizes the inputs to each layer to have zero mean and unit variance, with learnable scale and shift parameters."
  },
  {
    id: "u-net-convolutional",
    arxivId: "1505.04597",
    title: "U-Net: Convolutional Networks for Biomedical Image Segmentation",
    authors: ["Olaf Ronneberger", "Philipp Fischer", "Thomas Brox"],
    abstract: "There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently.",
    views: "487k",
    citations: "42.1k",
    category: "Computer Vision",
    field: "Computer Science",
    publishedDate: "18 May 2015",
    summary: "U-Net introduces a U-shaped architecture for image segmentation that has become the standard for biomedical image analysis.",
    methodology: "Uses a contracting path to capture context and a symmetric expanding path for precise localization with skip connections."
  },
  {
    id: "yolo-real-time",
    arxivId: "1506.02640",
    title: "You Only Look Once: Unified, Real-Time Object Detection",
    authors: ["Joseph Redmon", "Santosh Divvala", "Ross Girshick", "Ali Farhadi"],
    abstract: "We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities.",
    views: "1.1M",
    citations: "38.9k",
    category: "Computer Vision",
    field: "Computer Science",
    publishedDate: "9 Jun 2015",
    summary: "YOLO revolutionizes object detection by treating it as a single regression problem, enabling real-time performance.",
    methodology: "Single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation."
  },
  {
    id: "word2vec-efficient",
    arxivId: "1301.3781",
    title: "Efficient Estimation of Word Representations in Vector Space",
    authors: ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"],
    abstract: "We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks.",
    views: "694k",
    citations: "34.7k",
    category: "Natural Language Processing",
    field: "Computer Science",
    publishedDate: "16 Jan 2013",
    summary: "Word2Vec introduces efficient methods for learning high-quality word embeddings from large datasets using skip-gram and CBOW models.",
    methodology: "Uses shallow neural networks to learn word embeddings through either continuous bag-of-words (CBOW) or skip-gram architectures."
  },
  {
    id: "alphago-mastering",
    arxivId: "1712.01815",
    title: "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm",
    authors: ["David Silver", "Thomas Hubert", "Julian Schrittwieser", "Ioannis Antonoglou", "Matthew Lai", "Arthur Guez", "Marc Lanctot", "Laurent Sifre", "Dharshan Kumaran", "Thore Graepel", "Timothy Lillicrap", "Karen Simonyan", "Demis Hassabis"],
    abstract: "The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades.",
    views: "534k",
    citations: "12.8k",
    category: "Artificial Intelligence",
    field: "Computer Science",
    publishedDate: "5 Dec 2017",
    summary: "AlphaZero generalizes the AlphaGo approach to master chess, shogi, and Go through pure self-play reinforcement learning.",
    methodology: "Combines Monte Carlo tree search with deep neural networks trained through self-play without human knowledge."
  },
  {
    id: "clip-learning-visual",
    arxivId: "2103.00020",
    title: "Learning Transferable Visual Representations from Natural Language Supervision",
    authors: ["Alec Radford", "Jong Wook Kim", "Chris Hallacy", "Aditya Ramesh", "Gabriel Goh", "Sandhini Agarwal", "Girish Sastry", "Amanda Askell", "Pamela Mishkin", "Jack Clark", "Gretchen Krueger", "Ilya Sutskever"],
    abstract: "State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept.",
    views: "721k",
    citations: "15.2k",
    category: "Computer Vision",
    field: "Computer Science",
    publishedDate: "26 Feb 2021",
    summary: "CLIP learns visual concepts from natural language supervision, enabling zero-shot transfer to many computer vision tasks.",
    methodology: "Trains an image encoder and text encoder to predict which images were paired with which texts in a dataset of 400 million (image, text) pairs."
  },
  {
    id: "stable-diffusion",
    arxivId: "2112.10752",
    title: "High-Resolution Image Synthesis with Latent Diffusion Models",
    authors: ["Robin Rombach", "Andreas Blattmann", "Dominik Lorenz", "Patrick Esser", "Björn Ommer"],
    abstract: "By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data. Additionally, their formulation allows for a guiding mechanism to control the generation process without retraining.",
    views: "612k",
    citations: "8.9k",
    category: "Computer Vision",
    field: "Computer Science",
    publishedDate: "20 Dec 2021",
    summary: "Stable Diffusion introduces latent diffusion models that generate high-quality images efficiently by working in a compressed latent space.",
    methodology: "Applies diffusion models in the latent space of a pretrained autoencoder, significantly reducing computational requirements while maintaining quality."
  },
  {
    id: "llama-open-efficient",
    arxivId: "2302.13971",
    title: "LLaMA: Open and Efficient Foundation Language Models",
    authors: ["Hugo Touvron", "Thibaut Lavril", "Gautier Izacard", "Xavier Martinet", "Marie-Anne Lachaux", "Timothée Lacroix", "Baptiste Rozière", "Naman Goyal", "Eric Hambro", "Faisal Azhar", "Aurelien Rodriguez", "Armand Joulin", "Edouard Grave", "Guillaume Lample"],
    abstract: "We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets.",
    views: "423k",
    citations: "6.7k",
    category: "Machine Learning",
    field: "Computer Science",
    publishedDate: "27 Feb 2023",
    summary: "LLaMA demonstrates that high-performance language models can be trained using only publicly available data, democratizing access to large language models.",
    methodology: "Trains transformer-based language models of various sizes on a carefully curated dataset of publicly available text sources."
  }
];

// Categories with counts and colors
export const categories: Category[] = [
  { id: "all", name: "All", count: papers.length, color: "gray" },
  { id: "machine-learning", name: "Machine Learning", count: papers.filter(p => p.category === "Machine Learning").length, color: "purple" },
  { id: "computer-vision", name: "Computer Vision", count: papers.filter(p => p.category === "Computer Vision").length, color: "blue" },
  { id: "natural-language-processing", name: "Natural Language Processing", count: papers.filter(p => p.category === "Natural Language Processing").length, color: "green" },
  { id: "artificial-intelligence", name: "Artificial Intelligence", count: papers.filter(p => p.category === "Artificial Intelligence").length, color: "orange" },
  { id: "deep-learning", name: "Deep Learning", count: papers.filter(p => p.category === "Deep Learning").length, color: "red" }
];
